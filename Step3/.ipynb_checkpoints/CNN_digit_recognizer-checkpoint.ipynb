{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check this -> https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten, MaxPool2D\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "import keras.optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "\n",
    "    \n",
    "    #create figure with 3x3 sub-plots\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].reshape((28,28)), cmap='binary')\n",
    "\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#scale define, makes easier for network to learn\n",
    "#x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "#x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000,28,28,1)\n",
    "\n",
    "\n",
    "\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    (thresh, im_bw) = cv2.threshold(x_train[i], 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    x_train[i] = im_bw.reshape((28,28,1))\n",
    "    \n",
    "for i in range(len(x_test)):\n",
    "    (thresh, im_bw) = cv2.threshold(x_test[i], 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    x_test[i] = im_bw.reshape((28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "23936/60000 [==========>...................] - ETA: 44s - loss: 1.2993 - acc: 0.7379"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-15b3bbe99797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m              metrics=['accuracy'])\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Model 1\n",
    "\n",
    "model = Sequential()\n",
    "#args: (filters, window size, input_shape)\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "#model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3,3), activation='relu'))\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=12, batch_size=128, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 : see: https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "# Define the optimizer\n",
    "#optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'RMSprop' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "epochs = 30 \n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (x_test,y_test),\n",
    "                              verbose = 2, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('binary_model_9842.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fac3c6bba90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9243486e-01, 9.3047375e-12, 7.0941322e-05, 3.6346883e-06,\n",
       "        2.2392879e-04, 1.1625448e-01, 7.8232239e-07, 3.9101079e-01,\n",
       "        5.4941461e-07, 4.3834172e-12]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.resize(img, (1,28,28,1))\n",
    "model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
